{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43edd212-58e4-447a-8185-a12eb391e70c",
   "metadata": {},
   "source": [
    "# Recitation 4: Backpropagation\n",
    "_Date_: 09/25/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0c368-0c22-41f0-88d6-6d21fdc6ef53",
   "metadata": {},
   "source": [
    "## Implement feedforward and backpropagation using `numpy` from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914a198-7bb2-4e49-b732-3b96f9342a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f08c6-c178-43ac-9f98-370ac4113808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# DO NOT MODIFY THIS CELL\n",
    "# =======================\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "\n",
    "def sigmoid_deriv(z):\n",
    "    return z * (1-z)\n",
    "\n",
    "\n",
    "def softmax(X, theta = 1.0, axis = None):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis = axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584ba1d-daa2-4302-9b19-f63e4508439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given both input matrix and truth matrix\n",
    "x = np.array([\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 1, 0, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328d03a-f369-4e7e-8a05-01f03edc1632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions for input and label matrices\n",
    "n, d = x.shape\n",
    "_, c = y.shape\n",
    "\n",
    "# Hidden dimensions\n",
    "h1, h2, h3 = 8, 6, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a2af73-fcb8-43c9-a661-38b70e8421a1",
   "metadata": {},
   "source": [
    "### Exercise: Fill in the dimensions for weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7cc98-64d8-4c59-96d1-b3f619b36a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing 3 weight matrices in the neural net\n",
    "w1 = np.ones((h1, d))\n",
    "w1[1, range(6)] = 0.3\n",
    "w1[3, range(6)] = 0.5\n",
    "\n",
    "# Second weight matrix\n",
    "w2 = np.ones((h2, h1))\n",
    "w2[2,range(5)] = 0.2\n",
    "w2[0, range(5)] = 0.6\n",
    "\n",
    "# Third weight matrix\n",
    "w3 = np.ones((h3, h2))\n",
    "w3[1, range(4)] = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a85cc8-3d00-42de-b349-054b81b041a7",
   "metadata": {},
   "source": [
    "### Exercise: Implement feedforward pass\n",
    "Given the input matrix `X`, implement\n",
    "* 2 hidden score matrices `z1`, `z2`\n",
    "* Logits `o` and\n",
    "* a predicted score matrix `y_hat` for each class\n",
    "\n",
    "using weight matrices and sigmoid function as the activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b39d3-1891-4dfa-be70-2bf896dabeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = sigmoid(x @ w1.T)\n",
    "z2 = sigmoid(z1 @ w2.T)\n",
    "o = z2 @ w3.T\n",
    "y_hat = softmax(o, axis=1)\n",
    "\n",
    "print(f\"After softmax:\\n{'-' * 20}\\n{y_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5afea-5001-45c0-b1d6-481ff846babc",
   "metadata": {},
   "source": [
    "### Exercise: Implement backpropagation pass\n",
    "You need to implement three components in the backpropagation pass (in sequence):\n",
    "* Error signal $D^{(i)}$\n",
    "* Cache matrix that combines current error signal and associated weight matrix $$C = D \\cdot \\Theta$$\n",
    "* Gradient matrix w.s.t specific weight using cache matrix and hidden matrix $$\\nabla \\Theta = D^T \\cdot Z = (F'(Z) \\odot C)^T \\cdot Z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ceb687-3b84-43a5-9d15-954ad937d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last layer: loss -> W3\n",
    "D_o = y_hat - y\n",
    "cache_z2 = D_o @ w3\n",
    "grad_w3 = D_o.T @ z2\n",
    "\n",
    "# Second layer\n",
    "D_2 = sigmoid_deriv(z2) * cache_z2\n",
    "cache_z1 = D_2 @ w2\n",
    "grad_w2 = D_2.T @ z1\n",
    "\n",
    "# First layer\n",
    "D_1 = sigmoid_deriv(z1) * cache_z1\n",
    "grad_w1 = D_1.T @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b21c0f-9511-437e-9fb5-7794d02fba1e",
   "metadata": {},
   "source": [
    "### Exercise: Implement weight update\n",
    "Assume using Gradient Descent, and the learning rate is fixed to $0.1$, update the weight for next iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd471ed-86ac-4f2d-956a-b4d7e5bc3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "new_w1 = w1 - 0.1 * grad_w1\n",
    "new_w2 = w2 - 0.1 * grad_w2\n",
    "new_w3 = w3 - 0.1 * grad_w3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93ad5a-960d-4ac4-90b2-a6939400bd85",
   "metadata": {},
   "source": [
    "## Implement using `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afa55d-9c3f-4e88-a19e-63cf5f9de4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369375ca-8013-41b6-bad2-7b5e8ee8f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = torch.tensor(x, dtype=torch.float)\n",
    "y_ = torch.tensor(y, dtype=torch.float)\n",
    "w1_ = torch.tensor(w1, dtype=torch.float, requires_grad=True)\n",
    "w2_ = torch.tensor(w2, dtype=torch.float, requires_grad=True)\n",
    "w3_ = torch.tensor(w3, dtype=torch.float, requires_grad=True)\n",
    "F = nn.Sigmoid()\n",
    "softmax_fn = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd61f4-7de0-4271-85cd-7b8f5bb1aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_ = F(x_ @ w1_.T)\n",
    "z2_ = F(z1_ @ w2_.T)\n",
    "o_ = z2_ @ w3_.T\n",
    "y_hat_ = softmax_fn(o_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be93b55-758b-4b2b-bf3c-d919512099a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc77d05-f7fc-46e2-8124-a7173895656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = nn.functional.cross_entropy(o_, y_)\n",
    "ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21550ad-e6a1-4fd8-b5fe-ce8e13a51e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb3bc6-126a-47c7-ba38-b8dab4784ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(w1_.grad - torch.tensor(grad_w1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
