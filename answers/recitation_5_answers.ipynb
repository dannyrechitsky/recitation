{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93acf17f-3cd9-4f09-b457-169928340af5",
   "metadata": {},
   "source": [
    "# Recitation 5: Dense embedding and CNN\n",
    "\n",
    "_Date_: 10/9/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c8832-0fbd-4c31-b5c5-6bd9bad68d39",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this recitation, we use _Names_ corpus whose each instance is a tuple of (name, gender).\n",
    "\n",
    "The original dataset is stored in a directory which has following structure:\n",
    "```\n",
    ">>> tree ./data/names\n",
    "\n",
    "data/names\n",
    "├── female.txt\n",
    "├── male.txt\n",
    "└── README\n",
    "```\n",
    "\n",
    "_**TODO**_: download the data and put them in a newly made directory called `data` in this recitation project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86506572-7cf0-4b6c-a8f6-7fa88cc01a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61461cd-5baf-488c-a55c-7dc011094a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NameInstance:\n",
    "    \"\"\"Dataclass for a single instance of 'Names' dataset\"\"\"\n",
    "    name: str\n",
    "    gender: str\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"< Name: {self.name} | Gender: {self.gender} >\"\n",
    "    \n",
    "\n",
    "def load_names(data_dir: str) -> List[NameInstance]:\n",
    "    \"\"\"Load instances of 'Names' dataset\"\"\"\n",
    "    raw_data = []\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if not filename.endswith('.txt'):\n",
    "            continue\n",
    "\n",
    "        data_file = os.path.join(data_dir, filename)\n",
    "        label = os.path.splitext(filename)[0]\n",
    "\n",
    "        with open(data_file, \"r\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                # raw_data.append([line.strip(), label])\n",
    "                raw_data.append(NameInstance(name=line.strip(), gender=label))\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def split_train_test(\n",
    "    data: List[NameInstance],\n",
    "    train_size: int,\n",
    "    seed: int = 42\n",
    ") -> Tuple[List[NameInstance], List[NameInstance]]:\n",
    "    \"\"\"Split the data into train and test set\"\"\"\n",
    "    assert train_size < len(data), 'training size must be less than the whole set'\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    return data[:train_size], data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a6901e-baf2-4bb8-b507-e55419f5c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Name: Aaron | Gender: male >\n"
     ]
    }
   ],
   "source": [
    "names = load_names('../data/names')\n",
    "sample_name = names[1]\n",
    "print(sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa45a4c-e039-4eef-b8ad-1d6cc92b2820",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names, test_names = split_train_test(names, 6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b630a16-e593-483e-ac2d-4e9b1f8e5b92",
   "metadata": {},
   "source": [
    "## Dense embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58832c5-9fcc-41a7-b994-0edfa8155456",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "Even though it's doable simply learning the relationship between the gender and the name at word-level, recall that a string can be seen as a list of characters. A character-level representation may capture more features from a single name, for example, the order, frequency of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1501ae4-f19e-48e1-9a69-8d7f13771c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239f3138-c987-4db5-b64a-566344070ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(\n",
    "    train_instances: List[NameInstance],\n",
    "    special_tokens=None,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"Build vocabulary from names in the train set\"\"\"\n",
    "    vocab_set = set()\n",
    "    \n",
    "    if special_tokens:\n",
    "        vocab_set.update(special_tokens)\n",
    "    else:\n",
    "        special_tokens = set()\n",
    "    \n",
    "    for inst in train_instances:\n",
    "        name = inst.name.lower()\n",
    "        vocab_set.update(set(name))\n",
    "\n",
    "    idx = 0\n",
    "    vocab = {}\n",
    "    \n",
    "    for char in vocab_set:\n",
    "        if char.isalpha() or char in special_tokens:\n",
    "            vocab[char] = idx\n",
    "            idx += 1\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def build_label_map(name_instances: List[NameInstance]) -> Dict[str, int]:\n",
    "    \"\"\"Build label map from the whole dataset\"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    label_count = Counter([instance.gender.lower() for instance in name_instances])\n",
    "    \n",
    "    return {label: idx for idx, label in enumerate(label_count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f3fea4-efd7-47e4-8111-8e2698306388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_token, unk_token = '@', '#'\n",
    "V = build_vocab(train_names, special_tokens=[pad_token, unk_token])\n",
    "label_map = build_label_map(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cb1cb6-5d68-42a5-a47f-19bf6a1ca9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'female': 0, 'male': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ab081f-5b9d-4ea9-b6cc-8f35c83cc3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': 0,\n",
       " 'x': 1,\n",
       " 'h': 2,\n",
       " 'y': 3,\n",
       " 'b': 4,\n",
       " 'q': 5,\n",
       " 'p': 6,\n",
       " 'z': 7,\n",
       " 'd': 8,\n",
       " 'l': 9,\n",
       " 'c': 10,\n",
       " 'i': 11,\n",
       " 'e': 12,\n",
       " 's': 13,\n",
       " '#': 14,\n",
       " 'm': 15,\n",
       " 'n': 16,\n",
       " 'g': 17,\n",
       " 'u': 18,\n",
       " 'v': 19,\n",
       " 't': 20,\n",
       " 'a': 21,\n",
       " 'j': 22,\n",
       " 'r': 23,\n",
       " 'w': 24,\n",
       " 'k': 25,\n",
       " '@': 26,\n",
       " 'o': 27}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6b630-07bf-46fe-bca9-65a025b11876",
   "metadata": {},
   "source": [
    "### Dense vectorization\n",
    "In general, we can regard the vectorization as a two-step transformation process. We need two operators $I$ and $T$, where $I$ access a token's ID and $T$ transforms a token's ID (basic unit) to a vector (i.e. 1d tensor). $$\\vec{v} = T(I(t))$$\n",
    "\n",
    "Previously in one-hot encoding, the transformation is divided into two sub-steps:\n",
    "1. Access the token's ID $i$ in the vocabulary\n",
    "2. Transform the integer ID $i$ to a vector by assigning $1$ at $i^{th}$ position while assigning $0$ for rest of positions.\n",
    "\n",
    "Similarly, in dense vectorization, the only difference is that the vector $\\vec{v}$ is different. It is dense which means it has much fewer dimensions that a sparse vector, so it reflects semantic similarity more easily.\n",
    "\n",
    "That's to say, $T_{\\text{dense}}$ has different outcome than $T_{\\text{sparse}}$. We learned (static) word embeddings from the lecture, so each token in the vocabulary of the training paradigm has a corresponding dense vector with dimension $d$.\n",
    "\n",
    "[`nn.Embedding`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html) implements $T_{\\text{dense}} \\in \\mathbb{R}^{|V| \\times d}$ for you so we can use that in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b102d7d-5840-4ec7-b098-7a3ff9a22e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa5695a-1278-419b-94df-f7b968d9f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(name: str) -> List[str]:\n",
    "    \"\"\"Tokenize a name string to a list of chars\"\"\"\n",
    "    return list(name.lower())\n",
    "    \n",
    "\n",
    "def dense_map(\n",
    "    name_instance: NameInstance,\n",
    "    vocab: Dict[str, int],\n",
    "    label_map: Dict[str, int],\n",
    "    max_len: int,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Map a name string to a list of indices and its gender index\"\"\"\n",
    "    chars = tokenize(name_instance.name.lower())\n",
    "\n",
    "    if len(chars) > max_len:\n",
    "        new_chars = chars[:max_len]\n",
    "    else:\n",
    "        new_chars = chars + [pad_token] * (max_len - len(chars))\n",
    "    \n",
    "    indices, label = [vocab.get(char, vocab[unk_token]) for char in new_chars], label_map[name_instance.gender.lower()]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Original chars: {chars}\\nAfter: {new_chars}\")\n",
    "    \n",
    "    return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "755f03ba-6e42-4c3e-a6ab-ad27939b204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original chars: ['b', 'o', 'h', 'a', 'n']\n",
      "After: ['b', 'o', 'h', 'a', 'n', '@', '@', '@', '@', '@']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 4, 27,  2, 21, 16, 26, 26, 26, 26, 26]), tensor(1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_map(NameInstance('Bohan', 'male'), V, label_map, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13032c73-c05e-4fe6-b17e-9299c32e8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        names: List[NameInstance],\n",
    "        vocab: Dict[str, int],\n",
    "        label_map: Dict[str, int],\n",
    "        max_len: int\n",
    "    ):\n",
    "        self.names = names\n",
    "        self.vocab = vocab\n",
    "        self.label_map = label_map\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[List[int], int]:\n",
    "         return dense_map(self.names[idx], self.vocab, self.label_map, self.max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d31a1a-fd8a-45aa-9e35-6ddff6070581",
   "metadata": {},
   "source": [
    "## CNN (or ConvNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d542a2d-e9a9-49c1-803b-03cc3cbf8292",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cc17c-df17-4b46-a8ec-cc2b28a58206",
   "metadata": {},
   "source": [
    "By its name, CNN is a neural network mainly operating image convolutions (as well as other important operations like pooling, activation functions, etc.).\n",
    "\n",
    "The convolution can be expressed as\n",
    "$$Y = W * X$$\n",
    "where $W$ is called _kernel_ (or _filter_) and $X$ is the image to be transformed.\n",
    "\n",
    "To better understand the convolution, it can be seen as a series of dot-product operations. The kernel is a sliding magnifier that specifically extract features from where it covers. Therefore, within its entire motions (i.e. convolution), the kernel can be configured in two ways:\n",
    "* What to see => padding & dilation \n",
    "* How to move => stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02700dcf-7aae-43f0-a0dd-877622f7c733",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example\n",
    "\n",
    "```\n",
    "1.     Embedding          Kernel\n",
    "\n",
    "    We     : -1, 0, 1     -1, 1\n",
    "    are    :  0, 1, 2      1, 0\n",
    "    here   :  1, 2, 3      0, 1\n",
    "    .      :  1, 1, 1\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "2.  Embedding Transpose      Kernel\n",
    "\n",
    "    We  are   here   .\n",
    "    -1   0     1     1       -1, 1\n",
    "     0   1     2     1        1, 0\n",
    "     1   2     3     1        0, 1\n",
    "```\n",
    "\n",
    "```\n",
    "3. Convolution #1\n",
    "\n",
    "    Position of window: [We are] here .\n",
    "    \n",
    "    We  are     Kernel\n",
    "    -1   0      -1, 1      1 0\n",
    "     0   1   x   1, 0   =  0 0  => sum: 3\n",
    "     1   2       0, 1      0 2\n",
    "```\n",
    "\n",
    "```\n",
    "4. Convolution #2\n",
    "\n",
    "    Position of window: We [are here] .\n",
    "    \n",
    "    are  here      Kernel\n",
    "     0     1       -1, 1      0 1\n",
    "     1     2    x   1, 0   =  1 0  => sum: 5\n",
    "     2     3        0, 1      0 3\n",
    "```\n",
    "\n",
    "```\n",
    "5. Convolution #3\n",
    "\n",
    "        Position of window: We are [here .]\n",
    "        \n",
    "     here   .      Kernel\n",
    "      1     1      -1, 1     -1 1\n",
    "      2     1   x   1, 0   =  2 0  => sum: 3\n",
    "      3     1       0, 1      0 1\n",
    "```\n",
    "\n",
    "```\n",
    "6. Output\n",
    "\n",
    "    3\n",
    "    5\n",
    "    3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8fc3f-cb6c-4e03-80e2-908030035f6a",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "PyTorch provides two APIs for convolution: `conv1d` and `conv2d`.\n",
    "\n",
    "#### `conv1d`\n",
    "It operates cross-correlation (which measures similarity between two signals) for one-dimensional data, for example, time-series, a sequence of word embeddings.\n",
    "\n",
    "INPUT: `(batch_size, in_embed_dim, in_seq_len)`\n",
    "\n",
    "OUTPUT: `(batch_size, out_embed_dim, out_seq_len)`\n",
    "\n",
    "---\n",
    "\n",
    "Arguments:\n",
    "* `in_channels`: the size of the input embeddings\n",
    "* `out_channels`: the size of the output feature vector (in FFN, the size of next hidden layer)\n",
    "* `kernel_size`: the number of tokens to filter in a single convolution\n",
    "  * e.g. $2$ stands for bigrams, $3$ stands for trigrams\n",
    "\n",
    "\n",
    "#### `conv2d`\n",
    "Similar to `conv1d` but operates on two-dimensional data, for example, images or any other grid-like data.\n",
    "\n",
    "INPUT: `(batch_size, in_embed_dim, height, width)`\n",
    "\n",
    "OUTPUT: `(batch_size, out_embed_dim, out_height, out_width)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "637f19cd-4424-4bae-b2e8-87c01749431e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.,  0.,  1.],\n",
       "          [ 0.,  1.,  2.],\n",
       "          [ 1.,  2.,  3.],\n",
       "          [ 1.,  1.,  1.]]]),\n",
       " torch.Size([1, 4, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_embeds = torch.tensor([[\n",
    "    [-1, 0, 1],\n",
    "    [0, 1, 2],\n",
    "    [1, 2, 3],\n",
    "    [1, 1, 1]\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "sent_embeds, sent_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ddc7ee-6349-4ca9-ae30-bed9647290c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.,  0.,  1.,  1.],\n",
       "          [ 0.,  1.,  2.,  1.],\n",
       "          [ 1.,  2.,  3.,  1.]]]),\n",
       " torch.Size([1, 3, 4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_embeds_transpose = sent_embeds.transpose(1, 2)\n",
    "\n",
    "sent_embeds_transpose, sent_embeds_transpose.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bba93cec-0885-45ff-ab91-ff5f9000f09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[-1.,  1.],\n",
       "          [ 1.,  0.],\n",
       "          [ 0.,  1.]]], requires_grad=True),\n",
       " torch.Size([1, 3, 2]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = torch.tensor([[\n",
    "    [-1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1]\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "conv_op = nn.Conv1d(\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    kernel_size=2,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "conv_op.weight = nn.Parameter(kernel)\n",
    "\n",
    "conv_op.weight, conv_op.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49b0310a-f9b8-42a3-9bfb-dad6335fcc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[3., 5., 3.]]], grad_fn=<ConvolutionBackward0>),\n",
       " torch.Size([1, 1, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_out = conv_op(sent_embeds_transpose)\n",
    "\n",
    "conv_out, conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cde32be-64ca-4679-971b-b3fbabb72c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[3., 5., 3.]]], grad_fn=<ReluBackward0>), torch.Size([1, 1, 3]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activation function\n",
    "relu = nn.ReLU()\n",
    "conv_relu = relu(conv_out)\n",
    "conv_relu, conv_relu.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b4706-b699-4243-869b-ce67228801be",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "Pooling is another magnifier/filter following the convolution layer (and activation function) but behaves differently. It performs certain operation to values in a sliding window (say find maximum value) and discard rest. Therefore, as convolution, pooling would also change the shape of a feature representation.\n",
    "\n",
    "\n",
    "PyTorch provides below pooling strategies:\n",
    "* Max pooling\n",
    "* Mean pooling\n",
    "* Power avg. pooling\n",
    "* Adaptive max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db5660-60f4-40b3-b271-fba16154f9be",
   "metadata": {},
   "source": [
    "#### `nn.MaxPooling1d`\n",
    "Key arguments:\n",
    "* `kernel_size`: the size of the sliding window\n",
    "\n",
    "#### `nn.functional.max_pool1d`\n",
    "Key arguments:\n",
    "* `input`\n",
    "* `kernel_size`\n",
    "\n",
    "What's the difference?\n",
    "* `nn.MaxPooling1d` needs to be declared in `__init__`, and you need to calculate the \"length\" of the input\n",
    "* `nn.functional.max_pool1d` doesn't need to calculate in advance but retrieve the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8992d77e-93a2-4250-8c82-02a43f503bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[5.]]], grad_fn=<SqueezeBackward1>), torch.Size([1, 1, 1]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "conv_pool = F.max_pool1d(conv_relu, kernel_size=conv_relu.size(-1))\n",
    "\n",
    "conv_pool, conv_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36df41b0-3e33-4b89-a64b-c9f102eac241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.]], grad_fn=<SqueezeBackward1>), torch.Size([1, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_pool = conv_pool.squeeze(-1)\n",
    "\n",
    "conv_pool, conv_pool.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b039fb1-3fec-4d99-8d19-85812d46eb49",
   "metadata": {},
   "source": [
    "### Train CNN using Names corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71692ee5-74e5-4a39-9c98-a538f14eb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        kernel_size: int,\n",
    "        num_hidden: int,\n",
    "        num_class: int,\n",
    "        padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.conv = nn.Conv1d(embed_dim, num_hidden, kernel_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(num_hidden, num_class)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # input data's shape: (`batch_size`, `seq_len`) -> batch of indices\n",
    "        ### 1. embedding: turn indices into feature vectors\n",
    "        embeds = self.embedding(data)                # (`batch_size`, `seq_len`, `embed_dim`)\n",
    "        embeds = torch.transpose(embeds, 1, 2)       # (`batch_size`, `embed_dim`, `seq_len`)\n",
    "\n",
    "        ### 2. convolution and activation\n",
    "        conv = self.relu(self.conv(embeds))          # (`batch_size`, `num_hidden`, `seq_len` - `kernel_size` + 1)\n",
    "\n",
    "        ### 3. max pooling allows you to reduce the last dimension\n",
    "        conv = F.max_pool1d(conv, conv.size(-1))     # (`batch_size`, `num_hidden`, 1)\n",
    "        conv = torch.squeeze(conv, -1)               # (`batch_size`, `num_hidden`)\n",
    "\n",
    "        ### 4. final linear layer to output scores/logits over all possible labels\n",
    "        logits = self.linear(conv)                   # (`batch_size`, `num_class`)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42a7dc7-c312-405a-b82b-5b005e76f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# training\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# model\n",
    "EMBED_DIM = 32   # arbitrary embedding dimension\n",
    "NUM_HIDDEN = 16  # arbitrary number of hidden dimension\n",
    "KERNEL_SIZE = 3  # kernal size for CNN (3 meaning tri-gram)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d95dda5d-4623-4e03-8e27-88ad734dad3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(28, 32, padding_idx=26)\n",
       "  (conv): Conv1d(32, 16, kernel_size=(3,), stride=(1,))\n",
       "  (relu): ReLU()\n",
       "  (linear): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = CNN(\n",
    "    vocab_size=len(V),\n",
    "    embed_dim=EMBED_DIM,     \n",
    "    kernel_size=KERNEL_SIZE, \n",
    "    num_hidden=NUM_HIDDEN,   \n",
    "    num_class=len(label_map),\n",
    "    padding_idx=V[pad_token]\n",
    ")\n",
    "cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffa04417-450e-4abc-b739-d642756eb7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "912728ac-f1a9-4ff0-b3f0-a6d3f80f74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = NamesDataset(train_names, V, label_map, 6)\n",
    "test_ds = NamesDataset(test_names, V, label_map, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e73c7d33-fdf2-4af4-85c6-d3ad45cc6268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Loss: 122.064\n",
      "Epoch [2/20] | Loss: 117.113\n",
      "Epoch [3/20] | Loss: 114.083\n",
      "Epoch [4/20] | Loss: 111.724\n",
      "Epoch [5/20] | Loss: 109.222\n",
      "Epoch [6/20] | Loss: 107.089\n",
      "Epoch [7/20] | Loss: 105.120\n",
      "Epoch [8/20] | Loss: 103.465\n",
      "Epoch [9/20] | Loss: 101.934\n",
      "Epoch [10/20] | Loss: 100.661\n",
      "Epoch [11/20] | Loss: 99.608\n",
      "Epoch [12/20] | Loss: 98.312\n",
      "Epoch [13/20] | Loss: 97.543\n",
      "Epoch [14/20] | Loss: 96.575\n",
      "Epoch [15/20] | Loss: 95.615\n",
      "Epoch [16/20] | Loss: 94.856\n",
      "Epoch [17/20] | Loss: 94.193\n",
      "Epoch [18/20] | Loss: 93.532\n",
      "Epoch [19/20] | Loss: 92.506\n",
      "Epoch [20/20] | Loss: 92.018\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "training_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0  # total loss in any given epoch\n",
    "\n",
    "    for x, y in training_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()      # 1. clear out gradients from the last step\n",
    "\n",
    "        o = cnn(x)                 # 2. forward pass (this calls `forward` function of `clf`)\n",
    "        loss = loss_fn(o, y)       # 3. compute loss\n",
    "        loss.backward()            # 4. backward pass (i.e., computes gradients)\n",
    "\n",
    "        optimizer.step()           # 5. update weights\n",
    "\n",
    "        epoch_loss += loss.item()  # (optional) accumulate loss for the entire epoch\n",
    "    print('Epoch [{}/{}] | Loss: {:.3f}'.format(i+1, NUM_EPOCHS, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5da7007-ca1e-44e4-9956-3de4c880022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Acc: 0.727\n"
     ]
    }
   ],
   "source": [
    "# Evaluation / Inference / Prediction Loop\n",
    "test_dataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "with torch.no_grad(): # only during inference\n",
    "    num_correct = 0\n",
    "    for x, y in test_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = cnn(x)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        num_correct += (preds == y).sum()\n",
    "    acc = num_correct.item() / len(test_ds)\n",
    "    print(\"TEST Acc: {:.3f}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
