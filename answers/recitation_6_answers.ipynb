{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a55d38-1342-4d0b-b890-51bc3f0dd0e2",
   "metadata": {},
   "source": [
    "# Recitation 6: Project 2 preview\n",
    "_Date_: 2025-10-23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007563c4-2b49-4c68-bde2-4c6982e9b4c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Quick introduction\n",
    "\n",
    "- **Task**: Recover text eliminated from a preprocessing pipeline\n",
    "- **Model**: Transformer (Encoder-Decoder)\n",
    "- **Evaluation**: BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a967593-0fb6-41c9-8c7c-15fe1d9177f0",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "<figure>\n",
    "<img src=\"../assets/figures/transformer.png\" width=\"400\" align = \"center\"/>\n",
    "<figcaption align = \"center\"> Source: https://arxiv.org/pdf/1706.03762 </figcaption>\n",
    "</figure>\n",
    "\n",
    "### Components\n",
    "- Positional encoding\n",
    "- Embedding\n",
    "- Multihead attention\n",
    "  - Self attention\n",
    "  - Cross attention\n",
    "- Feedforward network\n",
    "- Residual connection & normalization\n",
    "- Masks\n",
    "  - key padding mask\n",
    "  - causal mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766ee19-78c4-4420-9bd8-63168fc814c4",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{PE}(pos, 2i) &=  \\sin \\Big( \\frac{pos}{10000^{2i / d_{m}}} \\Big) \\\\\n",
    "\\text{PE}(pos, 2i+1) &=  \\cos \\Big( \\frac{pos}{10000^{2i / d_{m}}} \\Big) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $pos$ is the position number in the sequence, and $i$ is the $i^{th}$ dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae2873-4cc3-4a36-b5e6-f2e9770bd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac6364-6c27-452b-966a-1f39efedc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding module\"\"\"\n",
    "    def __init__(self, d_model: int, seq_len: int):\n",
    "        \"\"\"\n",
    "        Assume a single data instance is a list of tokens\n",
    "\n",
    "        Args:\n",
    "            d_model (int): the size of the input embeddings\n",
    "            seq_len (int): the length of input sequence\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % 2 == 0, \"The embedding size must be divisible by 2\"\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000) / d_model)\n",
    "        )\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)  # even-numbered position\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)  # odd-numbered position\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # dim: (1, seq_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)  # PE will not be in state_dict and unlearnable\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward function\n",
    "        Args:\n",
    "            x (torch.Tensor): word embeddings for input sequence with the shape (B, seq_len, d_model)\n",
    "\n",
    "        Returns: new embeddings that add positional embeddings\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x_new = x + self.pe[:, :seq_len, :]\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb349afc-2815-410e-a15f-d88a24f68f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, embed_size = 1, 3, 8\n",
    "x = torch.rand(batch_size, seq_len, embed_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d9a24-5775-4efc-8f9f-08219c6fe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb = PosEncoding(embed_size, seq_len)\n",
    "res = pos_emb(x)\n",
    "print(f\"{res}\\nShape{res.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73822ff1-b16f-4df8-95b6-61cf43270841",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "<table><tr>\n",
    "    <td> <img src=\"../assets/figures/dot-prod-attn.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td> <img src=\"../assets/figures/mha.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Mathematically for scaled dot-product attention, $$Attention(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6afc13-43f7-47c4-854c-48b86fa55e72",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- Reshape tensor\n",
    "  - [`torch.Tensor.view(*shape)`](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.view.html): Reshape a (continguous) tensor with the same data without making a new copy\n",
    "- Masking\n",
    "  - [`torch.Tensor.masked_fill(mask, value)`](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html): Fill in the provided mask with a certain value to the tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d35a8-7e48-4116-af65-861eeb4bc548",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d9198-50cf-4484-b7ef-2065de2a5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padding_mask(seq: torch.Tensor, pad_idx=0):\n",
    "    \"\"\"\n",
    "    seq: (batch, seq_len)\n",
    "    Returns a mask (batch, seq_len) of booleans: True for non-pad tokens, False for pad.\n",
    "    \"\"\"\n",
    "    return seq != pad_idx\n",
    "\n",
    "\n",
    "def scale_dotprod_attn(\n",
    "    Q: torch.Tensor, \n",
    "    K: torch.Tensor, \n",
    "    V: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Scaled dot-product attention\n",
    "\n",
    "    Args:\n",
    "        Q: (target) input affine-transformed by W_Q, with shape (..., seq_len, d_Q)\n",
    "        K: (source) input affine-transformed by W_K, with shape (..., seq_len, d_K)\n",
    "        V: (source) input affine-transformed by W_V, with shape (..., seq_len, d_V)\n",
    "\n",
    "    Returns: attention outputs\n",
    "    \"\"\"\n",
    "    # 1. Compute matrix multiplication between Q and K followed by a scaling factor\n",
    "    scale = K.size(-1) ** -0.5\n",
    "    score = (Q @ K.transpose(-2, -1)) * scale\n",
    "\n",
    "    # 2. Optional masking\n",
    "    if mask is not None:\n",
    "        assert mask.dim() == score.dim(), 'Mask has different dimension as the scaled score between Q, K'\n",
    "        score = score.masked_fill(mask==0, -torch.inf)\n",
    "\n",
    "    # 3. Softmax and multiplies to V\n",
    "    attn = nn.functional.softmax(score, dim=-1)\n",
    "    out = attn @ V\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb9297-30fa-4e1f-9559-474057be69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scale_dotprod_attn(x, x, x)\n",
    "print(f\"{res}\\nShape: {res.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a40bc3-28a5-4d95-a315-10e51dfda193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    \"\"\"Multihead attention\"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        self.scale = self.d_head**-0.5\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask=None\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q: 'Query' matrix\n",
    "            k: 'Key' matrix\n",
    "            v: 'Value' matrix\n",
    "            mask: (optional) attention mask\n",
    "\n",
    "        Returns: a tuple of attention scores and attention output\n",
    "\n",
    "        \"\"\"\n",
    "        B, Tq, _ = q.size()\n",
    "        _, Tk, _ = k.size()\n",
    "\n",
    "        Q = self.w_q(q).view(B, Tq, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.w_k(k).view(B, Tk, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.w_v(v).view(B, Tk, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (B, heads, Tq, Tk)\n",
    "\n",
    "        if mask is not None:\n",
    "            assert mask.dim() == scores.dim(), (\n",
    "                f\"Provided mask has different dimension ({mask.dim()}) as QK scores ({scores.dim()})\"\n",
    "            )\n",
    "            scores = scores.masked_fill(mask == 0, -torch.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        out = torch.matmul(attn, V)  # (B, heads, Tq, d_head)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, Tq, self.num_heads * self.d_head)\n",
    "        out = self.w_o(out)\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ada9ce-8fe7-4256-805d-67afd0e98303",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MHA(embed_size, 2)\n",
    "o, _ = mha(x, x, x)\n",
    "print(f\"{o}\\nShape: {o.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be11c9-0fa1-4d66-b1bf-fa1a47ec5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \"\"\"Feedforward neural network\"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(d_model, d_ff)\n",
    "        self.lin2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = F.relu(self.lin1(x))\n",
    "        x2 = self.dropout(x2)\n",
    "        x2 = self.lin2(x2)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea2ef2-77d4-43db-9299-e978c54e76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single layer of Transformer encoder\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MHA(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = FFN(d_model, d_ff, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, attn = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b02e49-a32e-43b5-9bae-7e8f038f5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int,\n",
    "        num_layers: int,\n",
    "        max_len: int,\n",
    "        pad_idx: int,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_emb = PosEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        B, T = input_ids.size()\n",
    "        \n",
    "        x = self.pos_emb(self.token_emb(input_ids))\n",
    "        mask = make_padding_mask(input_ids, pad_idx=self.pad_idx)  # (B, T)\n",
    "\n",
    "        # Expand mask for multi-head attention: (B, T) -> (B, 1, 1, T)\n",
    "        mask_exp = mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        attns = []\n",
    "        for layer in self.layers:\n",
    "            x, a = layer(x, mask_exp)\n",
    "            attns.append(a)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, attns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
