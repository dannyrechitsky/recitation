{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76465e7b-bb92-4506-8b69-201432b0d89c",
   "metadata": {},
   "source": [
    "# Recitation 1: PyTorch tutorial\n",
    "_Date_: 2025-09-04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355712a3-625b-43c5-90ee-cb66303e0c64",
   "metadata": {},
   "source": [
    "## Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00542d1-7389-4170-be04-77bbc0d09a6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Introduction\n",
    "In a computational perspective, a tensor is a data structure that represents a multi-dimensional array. Terms \"vector\", \"matrix\" sound more familiar than \"tensor\". In fact, \"tensor\" is a more general term describing both vector and matrix. That is,\n",
    "\n",
    "* vector is a one-dimensional tensor $\\mathbb{R}^{n}$: $$\\mathbf{x} = [1, 5, 9]$$\n",
    "* matrix is a two-dimensional tensor $\\mathbb{R}^{n \\times m}$: $$X = \\begin{bmatrix} 2 & 4 & 6 \\\\ 7 & 10 & 3\\end{bmatrix} = \\begin{bmatrix} \\text{---} & \\mathbf{x_1} & \\text{---} \\\\ \\text{---} & \\mathbf{x_2} & \\text{---} \\end{bmatrix}$$\n",
    "* A three-dimensional tensor is $\\mathbb{R}^{n \\times m \\times k}$\n",
    "\n",
    "### PyTorch's tensor\n",
    "Implementation-wise, it's similar to Numpy's `ndarray`, but adds more features to adapt deep learning concepts:\n",
    "* `autograd` for back propagation updating weights of a neural net\n",
    "* store tensors in GPU memory for matrix multiplications in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834bda42-f594-45fa-9d3d-46fd79b7ec25",
   "metadata": {},
   "source": [
    "### Exercise: `torch` basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69b918b-921d-4b4e-8d9d-7e727f6eca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10872d12-5f9b-4a16-9a70-e55273a6120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v:\n",
      "-----\n",
      "tensor([0., 0., 0., 0., 0.]), shape:torch.Size([5]), dimensions: 1\n",
      "\n",
      "X:\n",
      "-----\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]), shape:torch.Size([5, 2]), dimensions: 2\n",
      "\n",
      "T:\n",
      "-----\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]]), shape:torch.Size([5, 2, 3]), dimensions: 3\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Intialize a tensor\n",
    "v = torch.zeros(5)  # vector\n",
    "X = torch.zeros(5,2)  # matrix\n",
    "T = torch.zeros(5,2,3)   # 3-dimensional tensor\n",
    "\n",
    "print(f\"v:\\n{'-' * 5}\\n{v}, shape:{v.shape}, dimensions: {v.ndim}\")\n",
    "print(f\"\\nX:\\n{'-' * 5}\\n{X}, shape:{X.shape}, dimensions: {X.ndim}\")\n",
    "print(f\"\\nT:\\n{'-' * 5}\\n{T}, shape:{T.shape}, dimensions: {T.ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cc652a9-5310-4f87-9cb5-20ab9a7515da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ones:\n",
      "-----\n",
      "tensor([1, 1, 1, 1, 1])\n",
      "\n",
      "zeros:\n",
      "-----\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "\n",
      "random:\n",
      "-----\n",
      "tensor([0.5980, 0.8485, 0.0442, 0.4108, 0.0837])\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Random or constant tensor\n",
    "ones_tensor = torch.full((5,), 1)  # tensor with all ones\n",
    "zeros_tensor = torch.zeros(5) # tensor with all zeros\n",
    "rand_tensor = torch.rand(5)  # tensor with random values\n",
    "\n",
    "print(f\"ones:\\n{'-' * 5}\\n{ones_tensor}\")\n",
    "print(f\"\\nzeros:\\n{'-' * 5}\\n{zeros_tensor}\")\n",
    "print(f\"\\nrandom:\\n{'-' * 5}\\n{rand_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02153fae-6bdb-4013-acf2-f855d3ea8a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "\n",
      "Data type of tensor: torch.float32\n",
      "\n",
      "Device of tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Get access to attributes of a tensor\n",
    "tensor = torch.zeros(3,4)  # Initialize a (3 x 4) tensor\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"\\nData type of tensor: {tensor.dtype}\")\n",
    "print(f\"\\nDevice of tensor: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4f44542-38d2-4192-ac66-da5237e256b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor as:\n",
      "--------------------\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "\n",
      "First row of tensor:\n",
      "--------------------\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "Last column of tensor:\n",
      "--------------------\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Indexing and slicing\n",
    "tensor = torch.zeros(4,4)  # Initialize a (4 x 4) tensor\n",
    "\n",
    "print(f\"Tensor as:\\n{'-' * 20}\\n{tensor}\")\n",
    "print(f\"\\nFirst row of tensor:\\n{'-' * 20}\\n{tensor[0]}\")\n",
    "print(f\"\\nLast column of tensor:\\n{'-' * 20}\\n{tensor[:, -1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a6118e6-a00d-456c-9394-360cff556578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x as:\n",
      "--------------------\n",
      "tensor([[0.5595, 0.3762, 0.2428, 0.5637],\n",
      "        [0.6477, 0.9716, 0.8429, 0.1957]])\n",
      "\n",
      "After masking:\n",
      "--------------------\n",
      "tensor([[0.5595, 0.0000, 0.2428, 0.5637],\n",
      "        [0.6477, 0.0000, 0.8429, 0.1957]])\n",
      "\n",
      "y as:\n",
      "--------------------\n",
      "tensor([[0.6036, 0.0696, 0.8351, 0.6708]])\n",
      "\n",
      "After stacking:\n",
      "--------------------\n",
      "tensor([[0.5595, 0.0000, 0.2428, 0.5637],\n",
      "        [0.6477, 0.0000, 0.8429, 0.1957],\n",
      "        [0.6036, 0.0696, 0.8351, 0.6708]])\n",
      "\n",
      " dimensions of stacked tensor:\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5: Manipulation\n",
    "x = torch.rand(2,4)  # Initialize a (2 x 4) tensor\n",
    "print(f\"x as:\\n{'-' * 20}\\n{x}\")\n",
    "\n",
    "x[:,1].zero_()      # Replace all values of the second column as 0 (masking)\n",
    "print(f\"\\nAfter masking:\\n{'-' * 20}\\n{x}\")\n",
    "\n",
    "y = torch.rand(1,4)  # Initialize a (1 x 4) tensor\n",
    "print(f\"\\ny as:\\n{'-' * 20}\\n{y}\")\n",
    "\n",
    "xy = torch.cat((x,y)) # Vertically stack x and y, resulting in a (3 x 4) tensor\n",
    "print(f\"\\nAfter stacking:\\n{'-' * 20}\\n{xy}\")\n",
    "\n",
    "print(f\"\\n dimensions of stacked tensor:\\n{xy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bf0cac1-0aa9-48be-8bde-897a5baab3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u:\n",
      "--------------------\n",
      "tensor([[ 0.0559, -0.3794,  1.5184,  0.6403],\n",
      "        [ 0.4333, -0.5603, -0.2830,  0.3149],\n",
      "        [ 0.0869,  1.7221,  0.4788,  1.5434]])\n",
      "\n",
      "v:\n",
      "--------------------\n",
      "tensor([[-0.2789,  0.3984],\n",
      "        [ 0.8057,  0.8326],\n",
      "        [-0.6939, -0.4142],\n",
      "        [-0.0186,  0.1564]])\n",
      "\n",
      "u @ v:\n",
      "--------------------\n",
      "tensor([[-1.3868, -0.8225],\n",
      "        [-0.3817, -0.1274],\n",
      "        [ 1.0023,  1.5116]])\n",
      "\n",
      "u * k:\n",
      "--------------------\n",
      "tensor([[-0.1500,  0.2362, -1.9582, -0.3361],\n",
      "        [ 0.0903,  0.2550,  0.0290,  0.0069],\n",
      "        [ 0.0156, -0.8477, -0.4300, -1.0642]])\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6: Arithimic operations\n",
    "u =  torch.randn(3,4) # Initialize a (3 x 4) tensor\n",
    "v = torch.randn(4,2)  # Initialize a (4 x 2) tensor\n",
    "print(f\"u:\\n{'-' * 20}\\n{u}\")\n",
    "print(f\"\\nv:\\n{'-' * 20}\\n{v}\")\n",
    "\n",
    "u_times_v = u @ v  # multiply u and v using operator ``@``\n",
    "u_times_v = torch.matmul(u, v)  # multiply u and v using instance function ``torch.matmul``\n",
    "print(f\"\\nu @ v:\\n{'-' * 20}\\n{u_times_v}\")\n",
    "\n",
    "k = torch.randn(3,4)  # Initialize a (3 x 4) tensor\n",
    "u_elem_times_k = u * k  # Compute element-wise product between u and k using ``*``\n",
    "u_elem_times_k = torch.mul(u,k)  # Compute element-wise product between u and k using ``torch.mul``\n",
    "print(f\"\\nu * k:\\n{'-' * 20}\\n{u_elem_times_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e6e68e-6738-4489-8b63-72f6c138f7df",
   "metadata": {},
   "source": [
    "## Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f21b82-f009-414b-a677-08fd81d6fd51",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Date preprocessing is one of the most fundamental and essential step among the standard pipeline training a neural network. Specifically, in NLP task, this step would do feature representation which translates a text data into numerical values. Commonly, this step does following things:\n",
    "* Clean and organize text data (e.g. remove punctuations, special characters and split dataset)\n",
    "* Featurize text to numerics, which may include below steps\n",
    "  * Build vocabulary from corpus\n",
    "  * Figure out how to numerically represent (or encode) a string given the vocabulary\n",
    "    * sparse encoding?\n",
    "    * dense encoding?\n",
    "  * Encode labels\n",
    "\n",
    "Eventually, implement above steps using python class or functions and integrate them with PyTorch's `torch.utils.data.Dataset` class.\n",
    "\n",
    "### PyTorch dataset and dataloaders\n",
    "`Dataset` can be thought as a list of data samples, which means it is indexed like a python list. Normally, this class includes the implementation of featurization, so accessing an individual sample returns numerical values. \n",
    "\n",
    "`DataLoader` wraps an iterable around a `Dataset` object so it's memory efficient in the training loop (recall python's generator), and it's capable of batchifying the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12488e6e-8336-40c5-b30a-1a1c7b84e964",
   "metadata": {},
   "source": [
    "### Exercise: preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af89387c-65d0-43bf-9d16-1fea15b15d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fec389c-72a6-4bd4-a4bc-ece42464457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below corpus is generated by AI\n",
    "corpus = \"\"\"\n",
    "The old bookstore on the corner smelled of paper and dust\n",
    "Have you ever wondered what lies beyond the farthest star\n",
    "Please hand me the blue folder from the top shelf\n",
    "Although the weather was cold, we enjoyed our walk along the beach\n",
    "What an incredible view from the mountaintop\n",
    "He practiced the piano for an hour every day; his dedication was admirable\n",
    "The new software update will be installed automatically tonight\n",
    "She brewed a cup of tea and watched the rain fall outside her window\n",
    "Innovation often arises from the intersection of different fields of study\n",
    "The children laughed as the puppy chased its tail in circles\n",
    "Can we reschedule our meeting for early next week\n",
    "The project was a success, but there were many challenges along the way\n",
    "\"\"\"\n",
    "\n",
    "# Generate synthetic labels\n",
    "labels = ['y', 'n', 'n', 'n', 'y', 'y', 'n', 'n', 'y', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "637c805a-e81c-4431-8181-9a844f4c43c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'old',\n",
       " 'bookstore',\n",
       " 'on',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'smelled',\n",
       " 'of',\n",
       " 'paper',\n",
       " 'and',\n",
       " 'dust']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 1: Implement functions for tokenizing a sentence into tokens\n",
    "def tokenize(instance: str) -> List[str]:\n",
    "    \"\"\"Tokenize a text data instance into a list features\"\"\"\n",
    "    tokens = instance.lower().strip().split()\n",
    "    return tokens\n",
    "\n",
    "tokenize(\"The old bookstore on the corner smelled of paper and dust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0dd9ece7-bc84-4940-a8e7-b09d9a16ab22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'old',\n",
       " 'bookstore',\n",
       " 'on',\n",
       " 'the',\n",
       " 'corner',\n",
       " 'smelled',\n",
       " 'of',\n",
       " 'paper',\n",
       " 'and',\n",
       " 'dust',\n",
       " 'have',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'wondered',\n",
       " 'what',\n",
       " 'lies',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'farthest',\n",
       " 'star',\n",
       " 'please',\n",
       " 'hand',\n",
       " 'me',\n",
       " 'the',\n",
       " 'blue',\n",
       " 'folder',\n",
       " 'from',\n",
       " 'the',\n",
       " 'top',\n",
       " 'shelf',\n",
       " 'although',\n",
       " 'the',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'cold,',\n",
       " 'we',\n",
       " 'enjoyed',\n",
       " 'our',\n",
       " 'walk',\n",
       " 'along',\n",
       " 'the',\n",
       " 'beach',\n",
       " 'what',\n",
       " 'an',\n",
       " 'incredible',\n",
       " 'view',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mountaintop',\n",
       " 'he',\n",
       " 'practiced',\n",
       " 'the',\n",
       " 'piano',\n",
       " 'for',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'every',\n",
       " 'day;',\n",
       " 'his',\n",
       " 'dedication',\n",
       " 'was',\n",
       " 'admirable',\n",
       " 'the',\n",
       " 'new',\n",
       " 'software',\n",
       " 'update',\n",
       " 'will',\n",
       " 'be',\n",
       " 'installed',\n",
       " 'automatically',\n",
       " 'tonight',\n",
       " 'she',\n",
       " 'brewed',\n",
       " 'a',\n",
       " 'cup',\n",
       " 'of',\n",
       " 'tea',\n",
       " 'and',\n",
       " 'watched',\n",
       " 'the',\n",
       " 'rain',\n",
       " 'fall',\n",
       " 'outside',\n",
       " 'her',\n",
       " 'window',\n",
       " 'innovation',\n",
       " 'often',\n",
       " 'arises',\n",
       " 'from',\n",
       " 'the',\n",
       " 'intersection',\n",
       " 'of',\n",
       " 'different',\n",
       " 'fields',\n",
       " 'of',\n",
       " 'study',\n",
       " 'the',\n",
       " 'children',\n",
       " 'laughed',\n",
       " 'as',\n",
       " 'the',\n",
       " 'puppy',\n",
       " 'chased',\n",
       " 'its',\n",
       " 'tail',\n",
       " 'in',\n",
       " 'circles',\n",
       " 'can',\n",
       " 'we',\n",
       " 'reschedule',\n",
       " 'our',\n",
       " 'meeting',\n",
       " 'for',\n",
       " 'early',\n",
       " 'next',\n",
       " 'week',\n",
       " 'the',\n",
       " 'project',\n",
       " 'was',\n",
       " 'a',\n",
       " 'success,',\n",
       " 'but',\n",
       " 'there',\n",
       " 'were',\n",
       " 'many',\n",
       " 'challenges',\n",
       " 'along',\n",
       " 'the',\n",
       " 'way']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for sentence in corpus.split('\\n'):\n",
    "    words.extend(tokenize(sentence))\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91e887e4-d434-4dbc-8708-c87f9363f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Implement functions for building vocabulary and label map\n",
    "\n",
    "def build_vocabulary(tokens: List[str], most_common: int) -> Dict[str, int]:\n",
    "    from collections import Counter\n",
    "    word_freq = Counter(tokens).most_common(most_common)\n",
    "    vocab = [word for word, _ in word_freq]\n",
    "    vocab.extend(['<PAD>', '<UNK>'])\n",
    "\n",
    "    return {w: i for i, w in enumerate(vocab)}\n",
    "        \n",
    "\n",
    "def build_label_map(labels: List[str]) -> Dict[str, int]:\n",
    "    label_set = set(labels)\n",
    "\n",
    "    return {label: i for i, label in enumerate(label_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c13d06e9-b89f-4732-8439-3efa1642274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with most frequent 5 words:\n",
      "--------------------------------------------------\n",
      "{'the': 0, 'of': 1, 'from': 2, 'was': 3, 'and': 4, '<PAD>': 5, '<UNK>': 6}\n",
      "\n",
      "Label map:\n",
      "--------------------------------------------------\n",
      "{'n': 0, 'y': 1}\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "vocab = build_vocabulary(words, k)\n",
    "label_map = build_label_map(labels)\n",
    "\n",
    "print(f\"Vocabulary with most frequent {k} words:\\n{'-' * 50}\\n{vocab}\")\n",
    "print(f\"\\nLabel map:\\n{'-' * 50}\\n{label_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a32460d7-ba73-478d-b495-cf774d9065ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement functions for featurizing processed data into numerical representations\n",
    "def to_sparse_vector(instance: str, vocab: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"Encode a sentence to a sparse vector by multi-hot encoding\"\"\"\n",
    "    # change sentence to set of tokens\n",
    "    token_set = set(instance.lower().split())\n",
    "    # if token in dict, add 1 at its index in the vector \n",
    "    vector = [0] * len(vocab)\n",
    "    for token in token_set:\n",
    "        if token in vocab:\n",
    "            vector[vocab[token]] = 1\n",
    "    return vector\n",
    "\n",
    "def to_dense_vector(instance: str, vocab: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"Encode a sentence as a list of integers\"\"\"\n",
    "    tokens = tokenize(instance)\n",
    "    vector = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61567ad8-c8c2-439b-8e35-8c0ab52d95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_vector = [to_sparse_vector(sentence, vocab) for sentence in corpus.strip().split('\\n')]\n",
    "dense_vector = [to_dense_vector(sentence, vocab) for sentence in corpus.strip().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b026c832-da6a-492e-a081-5e264d4025b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 1, 0, 0],\n",
       " [1, 1, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d07efc6f-ddaf-4bae-9adf-0692d6941a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 6, 6, 6, 0, 6, 6, 1, 6, 4, 6],\n",
       " [6, 6, 6, 6, 6, 6, 6, 0, 6, 6],\n",
       " [6, 6, 6, 0, 6, 6, 2, 0, 6, 6],\n",
       " [6, 0, 6, 3, 6, 6, 6, 6, 6, 6, 0, 6],\n",
       " [6, 6, 6, 6, 2, 0, 6],\n",
       " [6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6],\n",
       " [0, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [6, 6, 6, 6, 1, 6, 4, 6, 0, 6, 6, 6, 6, 6],\n",
       " [6, 6, 6, 2, 0, 6, 1, 6, 6, 1, 6],\n",
       " [0, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [0, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 0, 6]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca5795-4685-4b54-91f5-f182b77d137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement the custom dataset using ``Dataset``\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, corpus: str, labels: List[str], top_k: int):\n",
    "        ...\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647723a0-4ac4-4566-be12-000cca7d245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dataset = CustomDataset(corpus, labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857bb98-3a3f-4de8-8497-71fdbc63c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of the dataset is {len(corpus_dataset)}\")\n",
    "\n",
    "i = 3\n",
    "sp_v, ds_v, y = corpus_dataset[i]\n",
    "print(f\"\\nFor the {i+1} th element of the dataset:\\n{'-' * 50}\")\n",
    "print(f\"\\nSparse vector:\\n{'-' * 10}\\n{sp_v}\")\n",
    "print(f\"\\nDense vector:\\n{'-' * 10}\\n{ds_v}\")\n",
    "print(f\"\\nLabel:\\n{'-' * 10}\\n{y}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
