{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b385d4a4-4fdc-4890-b37e-9594a80f4f00",
   "metadata": {},
   "source": [
    "# Recitation 2: Build and train neural networks by PyTorch\n",
    "_Date_: 2025-09-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e5ce8-2027-4776-ad7a-ae1e448b2b4c",
   "metadata": {},
   "source": [
    "## Recall from data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e161b-62b1-42a4-bf84-b2f4c1135701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28aa503-0a30-4a7e-8d8a-5fe56f764504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below corpus is generated by AI\n",
    "corpus = \"\"\"\n",
    "The old bookstore on the corner smelled of paper and dust\n",
    "Have you ever wondered what lies beyond the farthest star\n",
    "Please hand me the blue folder from the top shelf\n",
    "Although the weather was cold, we enjoyed our walk along the beach\n",
    "What an incredible view from the mountaintop\n",
    "He practiced the piano for an hour every day; his dedication was admirable\n",
    "The new software update will be installed automatically tonight\n",
    "She brewed a cup of tea and watched the rain fall outside her window\n",
    "Innovation often arises from the intersection of different fields of study\n",
    "The children laughed as the puppy chased its tail in circles\n",
    "Can we reschedule our meeting for early next week\n",
    "The project was a success, but there were many challenges along the way\n",
    "\"\"\"\n",
    "\n",
    "# Generate synthetic labels\n",
    "labels = ['y', 'n', 'n', 'n', 'y', 'y', 'n', 'n', 'y', 'y', 'n', 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5c64d-654d-48dc-9ccc-d756b7d8df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(instance: str) -> List[str]:\n",
    "    \"\"\"Tokenize a text data instance into a list features\"\"\"\n",
    "    return instance.lower().strip().split()\n",
    "\n",
    "\n",
    "def build_vocabulary(tokens: List[str], most_common: int) -> Dict[str, int]:\n",
    "    from collections import Counter\n",
    "\n",
    "    word_freq = Counter(tokens).most_common(most_common)\n",
    "    vocab = [word for word, _ in word_freq]\n",
    "    vocab.extend(['<PAD>', '<UNK>'])\n",
    "\n",
    "    return {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def build_label_map(labels: List[str]) -> Dict[str, int]:\n",
    "    label_set = set(labels)\n",
    "\n",
    "    return {label: i for i, label in enumerate(label_set)}\n",
    "\n",
    "\n",
    "def to_sparse_vector(instance: str, vocab: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"Encode a sentence to a sparse vector by multi-hot encoding\"\"\"\n",
    "    features = tokenize(instance)\n",
    "\n",
    "    return [1 if elem in set(features) else 0 for elem in vocab]\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, corpus: str, labels: List[str], k: int):\n",
    "        # Step 1: Split corpus into sentences, and tokens\n",
    "        self.instances = corpus.strip().split('\\n')\n",
    "        self.labels = labels\n",
    "        tokens = []\n",
    "\n",
    "        for instance in self.instances:\n",
    "            tokens.extend(tokenize(instance))\n",
    "\n",
    "        # Step 2: Build vocabulary and label map\n",
    "        self.vocab = build_vocabulary(tokens, k)\n",
    "        self.label_map = build_label_map(labels)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        instance = self.instances[idx]\n",
    "        \n",
    "        label_val = torch.tensor(self.label_map[self.labels[idx]], dtype=torch.long)\n",
    "        \n",
    "        sparse_vec = torch.tensor(to_sparse_vector(instance, self.vocab), dtype=torch.float)\n",
    "\n",
    "        return sparse_vec, label_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44299f3c-13b0-42e9-b967-5eb082c38f7a",
   "metadata": {},
   "source": [
    "## Implement logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0237305-1aba-47e0-a7f3-57e4f527884a",
   "metadata": {},
   "source": [
    "### Review: Logistic regression\n",
    "Start with a linear model for predicting label $y$, where $y \\in \\cal{Y}$, $$z = \\vec{\\theta} \\cdot f(\\vec{x}, y)$$ where $z \\in \\mathbb{R}$, $\\vec{\\theta}$ is the weight vector and $f(\\vec{x}, y)$ is the feature vector.\n",
    "\n",
    "However, if we want a model that directly maps the feature vector $f(\\vec{x}, y)$ to a probability that falls in the range $[0,1]$?\n",
    "\n",
    "First, make the scalar score positive by exponentiation, $$z' = \\exp \\big({\\vec{\\theta} \\cdot f(\\vec{x}, y)}\\big)$$\n",
    "\n",
    "Then, normalize it to obtain a probability-like distribution, $$\\tilde{z} = \\frac{\\exp \\big({\\vec{\\theta} \\cdot f(\\vec{x}, y)}\\big)}{\\sum_{y' \\in \\cal{Y}} \\exp \\big({\\vec{\\theta} \\cdot f(\\vec{x}, y')}\\big)}$$\n",
    "\n",
    "* Linear discriminative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a549be-7d5f-4b23-a546-f712db0f5130",
   "metadata": {},
   "source": [
    "### `torch.nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37920c-6a53-46e2-b0a4-178a7af96a2a",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "`nn.Module` is the base class for implementing any module within in a neural network.\n",
    "\n",
    "In other words, any module, layer, functions that constitutes a neural network is a subclass of `nn.Module`.\n",
    "\n",
    "#### Implementation\n",
    "Basically, you need to implement two functions:\n",
    "* `__init__`: initialize the architecture, parameters for a neural network\n",
    "* `forward`: the function for forward pass\n",
    "\n",
    "There're some modules you need to know for PA0:\n",
    "* `nn.Linear`: apply linear transformation to the input tensor\n",
    "  * $A \\cdot \\Theta^{T} = B$\n",
    "  * Given the input tensor $A$ with shape $(m, n)$ and want the transformed tensor $B$ with shape $(m, k)$, what would be the shape of the $\\Theta^{T}$?\n",
    "  * How to apply it to linear model $\\vec{y} = \\Theta \\cdot \\vec{x}$?\n",
    "* `nn.Softmax`: apply softmax function to an n-dimensional tensor\n",
    "* `nn.Sequential`: a sequential module container which stores modules in the order they're passed in\n",
    "\n",
    "#### Modes\n",
    "A deep learning model acts two modes:\n",
    "* learning: model adjusts its weights for fitting seen data\n",
    "* inference: model estimates predicted values based on existing weights\n",
    "\n",
    "Recall from recitation 0 (@8/28/2025), PyTorch provides an architecture tracking and saving gradients for each parameter in a computational graph. Then it enables `nn.Module` class have \"learning\" and \"inference\" modes via instance functions:\n",
    "* `train()`: enable the model execute certain functions required for training (e.g. dropout)\n",
    "* `eval()`: disable those certains functions for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b77ba-c98e-44d7-8163-ee30fddad310",
   "metadata": {},
   "source": [
    "### Exercise: multinomial logistic regression using `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0c306-2f4e-4f2f-afe4-da1aa1221691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2fd89-c0ab-4123-8318-084c04edd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: implement multinomial logistic regression\n",
    "# NOTE: use ``nn.LogSoftmax`` after linear transformation\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim: int, out_dim: int):\n",
    "        ...\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002eddc-8176-4f44-82f8-15ce2ed5c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ds = CustomDataset(corpus, labels, 5)\n",
    "\n",
    "i = 5\n",
    "sparse_vec, label_val = corpus_ds[i]\n",
    "\n",
    "model = LogisticRegression(input_dim=len(corpus_ds.vocab), out_dim=len(corpus_ds.label_map))\n",
    "logits = model(sparse_vec)\n",
    "\n",
    "logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417fa79-e5e5-4706-b2e3-f05da728d518",
   "metadata": {},
   "source": [
    "## Optimization loop\n",
    "\n",
    "* The loop optimizes model parameters given training dataset.\n",
    "* An optimization loop consists of two sub-loops: train loop and test loop\n",
    "  * As many other kinds of loops, the optimization loop has several iterations called \"epochs\"\n",
    "* Each sub-loop has following components:\n",
    "  * Model\n",
    "  * Dataloader\n",
    "  * Loss/objective function\n",
    "  * Optimizer\n",
    "\n",
    "### Pseudocode\n",
    "```\n",
    "for batch in dataloader:\n",
    "    set the model in either train/eval mode\n",
    "    compute logits y_hat\n",
    "    compute loss between (y_hat) and (y from batch)\n",
    "\n",
    "    if train_loop:\n",
    "        loss function computes current gradients for weights\n",
    "        optimizer computes new weights based on current weights, current gradients and learning rate\n",
    "        optimizer resets/updates weights for the model\n",
    "\n",
    "    if test_loop:\n",
    "        make prediction by argmax(y_hat)\n",
    "```\n",
    "---\n",
    "### References\n",
    "* _Optimization loop_: https://docs.pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "* _Auto differentiation_: https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "* _Understand `optimizer.step()`_: https://medium.com/@whyamit404/what-does-optimizer-step-do-in-pytorch-83f0fb0cbfe5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578e941-ddf8-4fae-be40-26ebcc5b42c9",
   "metadata": {},
   "source": [
    "### Exercise: implement optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91375a04-9465-48c4-a5db-6b27e9be520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066808f-6fc4-4d23-9abc-44cfbc2ae5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: implement train loop\n",
    "\n",
    "def train_loop(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    loss_fn\n",
    "):\n",
    "    # set training mode\n",
    "\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        # compute logits\n",
    "\n",
    "        # compute loss\n",
    "\n",
    "        # compute current gradients by loss\n",
    "\n",
    "        # compute new gradients by optimizer\n",
    "\n",
    "        # reset current gradients\n",
    "\n",
    "        print(f\"Batch {i+1} loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f6ba4-2b13-4f9f-bf1c-e78331d38728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: implement test loop\n",
    "# NOTE: use ``torch.no_grad`` context for inference\n",
    "\n",
    "def test_loop(\n",
    "    model,\n",
    "    dataloader,\n",
    "    loss_fn\n",
    "):\n",
    "    # set evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # compute logits\n",
    "\n",
    "            # make prediction using argmax\n",
    "            \n",
    "            print(f\"True: {y.item()} | Prediction: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218637a-e56c-4811-9dc0-9da64711ea12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise: implement whole optimization loop by integrating both train and test loops\n",
    "#           use negative log-likelihood as loss function, and SGD as optimizer\n",
    "#\n",
    "# Useful modules:\n",
    "# ---\n",
    "# ``nn.NLLLoss``: negative log-likelihood loss\n",
    "# ``torch.optim.SGD``: SGD optimizer\n",
    "\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "\n",
    "ds = CustomDataset(corpus, labels, 5)\n",
    "model = LogisticRegression(input_dim=len(ds.vocab), out_dim=len(ds.label_map))\n",
    "\n",
    "train_ds, test_ds = random_split(ds, [0.8, 0.2])\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"\\nEpoch {i+1}\\n{'-' * 30}\")\n",
    "    train_loop(\n",
    "        model=...,\n",
    "        dataloader=...,\n",
    "        optimizer=...,\n",
    "        loss_fn=...\n",
    "    )\n",
    "    print('---')\n",
    "    test_loop(\n",
    "        model=...,\n",
    "        dataloader=...,\n",
    "        loss_fn=...\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
